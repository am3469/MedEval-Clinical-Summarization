# MedEval-Clinical-Summarization
This repository contains the implementation of *MedEval, a novel evaluation framework for assessing discharge summaries generated by large language models (LLMs) in clinical contexts. MedEval leverages **G-Eval, a GPT-4-based evaluation method, and applies it using **Chain-of-Thought (CoT) prompting* to measure clinical accuracy, completeness, readability, and actionability.

## ğŸ” Project Summary

Traditional evaluation metrics like ROUGE and BLEU fall short when applied to clinical text due to their focus on lexical overlap. *MedEval* addresses this gap by using a large language modelâ€“based evaluation framework that aligns closely with clinician expectations and medical relevance.

We evaluate summaries generated by *LLaMA-2-7B* and *Mistral-7B* models, using real ICU patient data from the *MIMIC-III* dataset.


## ğŸ§  Models Used

â€¢â   â *LLaMA-2-7B* (via Ollama)
â€¢â   â *Mistral-7B* (via Hugging Face)

These models were selected for their ability to handle structured prompts and generate coherent discharge summaries in a zero-shot setting.

## ğŸ“Š Evaluation Metrics (MedEval)

Each model-generated summary was scored on:
â€¢â   â *Clinical Accuracy*: Alignment with true diagnoses and procedures
â€¢â   â *Completeness*: Inclusion of all relevant medical details
â€¢â   â *Readability & Fluency*: Clarity, grammar, and narrative flow
â€¢â   â *Actionability*: Effectiveness of follow-up instructions

Scores were provided using a CoT-based rubric (0â€“10) per dimension.

## âš™ï¸ System Architecture

MedEval uses a 3-module pipeline:
1.â  â *Data Pipeline*: Preprocessing MIMIC-III dataset, extracting key clinical metadata
2.â  â *Model Pipeline*: Prompting LLMs with structured inputs
3.â  â *Evaluation Loop*: G-Evalâ€“based scoring using GPT-style rubric prompts

## ğŸ“ˆ Results Summary

â€¢â   â *Mistral-7B* performed better in *readability* and *actionability*
â€¢â   â *LLaMA2-7B* scored higher in *clinical accuracy* and *completeness*
â€¢â   â MedEval provided a more clinically meaningful assessment compared to BLEU/ROUGE

## ğŸ“š Dataset

â€¢â   â [MIMIC-III](https://www.kaggle.com/datasets/asjad99/mimiciii): Public ICU EHR dataset used for training and evaluation

## ğŸ§ª Future Work

â€¢â   â Incorporate *probabilistic scoring*
â€¢â   â Evaluate with clinically fine-tuned LLMs like *BioGPT, **ClinicalT5, and **MedAlpaca*
â€¢â   â Expand to include larger datasets and external evaluation partners

## ğŸ“„ License

This project is for academic use only. Refer to the respective licenses of the datasets and models used.

## ğŸ‘©â€ğŸ’» Authors

â€¢â   â Drashti Miteshkumar Mehta  
â€¢â   â Harini Varanasi  
â€¢â   â Akhila Madanapati  
â€¢â   â Anjali Bheemireddy

	â ğŸ“ Project submitted for *DTSC 5082 - Research Methodology*, University of North Texas.

