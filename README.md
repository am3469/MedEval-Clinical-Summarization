# MedEval-Clinical-Summarization
This repository contains the implementation of *MedEval, a novel evaluation framework for assessing discharge summaries generated by large language models (LLMs) in clinical contexts. MedEval leverages **G-Eval, a GPT-4-based evaluation method, and applies it using **Chain-of-Thought (CoT) prompting* to measure clinical accuracy, completeness, readability, and actionability.

## 🔍 Project Summary

Traditional evaluation metrics like ROUGE and BLEU fall short when applied to clinical text due to their focus on lexical overlap. *MedEval* addresses this gap by using a large language model–based evaluation framework that aligns closely with clinician expectations and medical relevance.

We evaluate summaries generated by *LLaMA-2-7B* and *Mistral-7B* models, using real ICU patient data from the *MIMIC-III* dataset.


## 🧠 Models Used

•⁠  ⁠*LLaMA-2-7B* (via Ollama)
•⁠  ⁠*Mistral-7B* (via Hugging Face)

These models were selected for their ability to handle structured prompts and generate coherent discharge summaries in a zero-shot setting.

## 📊 Evaluation Metrics (MedEval)

Each model-generated summary was scored on:
•⁠  ⁠*Clinical Accuracy*: Alignment with true diagnoses and procedures
•⁠  ⁠*Completeness*: Inclusion of all relevant medical details
•⁠  ⁠*Readability & Fluency*: Clarity, grammar, and narrative flow
•⁠  ⁠*Actionability*: Effectiveness of follow-up instructions

Scores were provided using a CoT-based rubric (0–10) per dimension.

## ⚙️ System Architecture

MedEval uses a 3-module pipeline:
1.⁠ ⁠*Data Pipeline*: Preprocessing MIMIC-III dataset, extracting key clinical metadata
2.⁠ ⁠*Model Pipeline*: Prompting LLMs with structured inputs
3.⁠ ⁠*Evaluation Loop*: G-Eval–based scoring using GPT-style rubric prompts

## 📈 Results Summary

•⁠  ⁠*Mistral-7B* performed better in *readability* and *actionability*
•⁠  ⁠*LLaMA2-7B* scored higher in *clinical accuracy* and *completeness*
•⁠  ⁠MedEval provided a more clinically meaningful assessment compared to BLEU/ROUGE

## 📚 Dataset

•⁠  ⁠[MIMIC-III](https://www.kaggle.com/datasets/asjad99/mimiciii): Public ICU EHR dataset used for training and evaluation

## 🧪 Future Work

•⁠  ⁠Incorporate *probabilistic scoring*
•⁠  ⁠Evaluate with clinically fine-tuned LLMs like *BioGPT, **ClinicalT5, and **MedAlpaca*
•⁠  ⁠Expand to include larger datasets and external evaluation partners

## 📄 License

This project is for academic use only. Refer to the respective licenses of the datasets and models used.

## 👩‍💻 Authors

•⁠  ⁠Drashti Miteshkumar Mehta  
•⁠  ⁠Harini Varanasi  
•⁠  ⁠Akhila Madanapati  
•⁠  ⁠Anjali Bheemireddy

	⁠📍 Project submitted for *DTSC 5082 - Research Methodology*, University of North Texas.

